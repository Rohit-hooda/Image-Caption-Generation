{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Captioning - Team SaaS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python2 and python3 compatibility between loaded modules\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All imports go here\n",
    "%matplotlib notebook\n",
    "\n",
    "# Reading files\n",
    "import os\n",
    "\n",
    "# Vector manipulations\n",
    "import numpy as np\n",
    "\n",
    "# DL framework\n",
    "# torch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch.utils.data as td\n",
    "import torchvision as tv\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "# Plotting images\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# COCO loading captions\n",
    "from pycocotools.coco import COCO\n",
    "import skimage.io as io\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (8.0, 10.0)\n",
    "\n",
    "# import created vocabulary\n",
    "from vocab_creator import VocabCreate as vc\n",
    "\n",
    "# PIL Image\n",
    "from PIL import Image\n",
    "\n",
    "# regex for captions\n",
    "import re\n",
    "\n",
    "# import nntools\n",
    "import nntools_modified as nt\n",
    "\n",
    "# import add for fast addition between lists\n",
    "from operator import add\n",
    "\n",
    "from pycocoevalcap.bleu.bleu import Bleu\n",
    "\n",
    "# json for dumping stuff onto files as output\n",
    "import json\n",
    "from json import encoder\n",
    "encoder.FLOAT_REPR = lambda o: format(o, '.3f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "dataset_root_dir = '/datasets/COCO-2015/'\n",
    "annotations_root_dir = '../datasets/COCO/annotations/'\n",
    "train_dir = \"train2014\"\n",
    "val_dir = \"val2014\"\n",
    "test_dir = \"test2015\"\n",
    "\n",
    "# output directory for training checkpoints\n",
    "# This changes for every experiment\n",
    "op_dir = \"../outputs/\" + <op_dir>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data annotations\n",
    "train_ann = \"{}captions_{}.json\".format(annotations_root_dir, train_dir)\n",
    "coco_train_caps = COCO(train_ann)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset class\n",
    "class COCODataset(td.Dataset):\n",
    "    \"\"\"Class to load the COCODataset\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_root_dir, annotations_root_dir, vocab, mode=\"train2014\", image_size=(224, 224)):\n",
    "        super(COCODataset, self).__init__()\n",
    "        self.dataset_root_dir = dataset_root_dir\n",
    "        self.annotations_root_dir = annotations_root_dir\n",
    "        self.image_size = image_size\n",
    "        self.mode = mode\n",
    "        # training data annotations\n",
    "        self.ann = \"{}captions_{}.json\".format(annotations_root_dir, mode)\n",
    "        self.coco_caps = COCO(self.ann)\n",
    "        # get all the image IDs\n",
    "        self.image_ids = self.coco_caps.getImgIds()\n",
    "        self.ann_ids = list(self.coco_caps.anns.keys())\n",
    "        # loadImgs() returns all the images\n",
    "        self.imgs = self.coco_caps.loadImgs(self.image_ids)\n",
    "        self.vocab = vocab\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ann_ids)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"COCODataset(mode={}, image_size={})\". \\\n",
    "        format(self.mode, self.image_size)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        ann_id = self.ann_ids[idx]\n",
    "        cap = self.coco_caps.anns[ann_id][\"caption\"]\n",
    "        img_id = self.coco_caps.anns[ann_id][\"image_id\"]\n",
    "        img_path = self.coco_caps.loadImgs(img_id)[0][\"file_name\"]\n",
    "        \n",
    "        img = Image.open('{}/{}/{}'.format(self.dataset_root_dir, self.mode, img_path))\n",
    "        img = img.convert('RGB')\n",
    "        transform = tv.transforms.Compose([\n",
    "            tv.transforms.Resize(self.image_size),\n",
    "            #tv.transforms.RandomHorizontalFlip(),\n",
    "            tv.transforms.ToTensor(),\n",
    "            tv.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "        x = transform(img)\n",
    "        \n",
    "        # return caption\n",
    "        cap = str(cap)\n",
    "        clean_cap = re.sub(r'[^a-zA-Z0-9 ]+', '', cap)\n",
    "        word_list = clean_cap.lower().strip().split()\n",
    "        for i in range(len(word_list)):\n",
    "            if word_list[i] not in vocab.one_hot_inds:\n",
    "                word_list[i]=\"unk_vec\"\n",
    "        d = torch.Tensor([vocab.one_hot_inds[\"start_vec\"]]\n",
    "                               + [vocab.one_hot_inds[w] for w in word_list]\n",
    "                               + [vocab.one_hot_inds[\"end_vec\"]]\n",
    "        )\n",
    "        return x, d\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load the vocabulary\n",
    "# or Create and save to output\n",
    "dict_path = \"../outputs/vocab.npz\"\n",
    "vocab = vc(train_ann, dict_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of the cocodataset\n",
    "training_dataset = COCODataset(dataset_root_dir, annotations_root_dir, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myimshow(image, ax=plt):\n",
    "    image = image.to('cpu').numpy()\n",
    "    image = np.moveaxis(image, [0, 1, 2], [2, 0, 1])\n",
    "    image = (image + 1) / 2\n",
    "    image[image<0] = 0\n",
    "    image[image>1] = 1\n",
    "    h = ax.imshow(image)\n",
    "    ax.axis('off')\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the dataloader to be used\n",
    "# collate_fn - to pad all the vectors to the same length\n",
    "def collate_function(data):\n",
    "    data.sort(key=lambda x:len(x[1]), reverse=True)\n",
    "    img, cap=zip(*data)\n",
    "\n",
    "    #stack images\n",
    "    img = torch.stack(img, 0)\n",
    "\n",
    "    #concatenate all captions\n",
    "    cap_lens = [len(c) for c in cap]\n",
    "    max_cap_lens = max(cap_lens)\n",
    "    cap_lens = torch.Tensor(cap_lens)\n",
    "\n",
    "    #pad all captions to max caption length\n",
    "    padded_caption = torch.zeros(len(cap),max_cap_lens).long()\n",
    "    for i, c in enumerate(cap):\n",
    "        c_len = int(cap_lens[i].item())\n",
    "        padded_caption[i,:c_len] = c[:c_len]\n",
    "\n",
    "    return img, padded_caption, cap_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = td.DataLoader(training_dataset, batch_size=128, shuffle=True, pin_memory=True,\n",
    "                             collate_fn=collate_function, worker_init_fn=torch.manual_seed(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index to caption\n",
    "def index_to_cap(labs):\n",
    "    \"\"\"Index to caption\"\"\"\n",
    "    cap = labs.cpu().data.numpy().astype(int)\n",
    "    caps = [vocab.dict[cap[c]] for c in range(len(cap))]\n",
    "    #caps = caps[1:-1]\n",
    "    caps = list(filter(lambda a: a != \"start_vec\", caps))\n",
    "    caps = list(filter(lambda a: a != \"end_vec\", caps))\n",
    "    caption = \" \".join(caps)\n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation dataset\n",
    "val_dataset = COCODataset(dataset_root_dir, annotations_root_dir, vocab, mode=val_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = td.DataLoader(val_dataset, batch_size=128, shuffle=False, pin_memory=True, collate_fn=collate_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN classifier from nntools\n",
    "class NNClassifier(nt.NeuralNetwork):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(NNClassifier, self).__init__()\n",
    "        self.cross_entropy = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def criterion(self, y, d):\n",
    "        return self.cross_entropy(y, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_RNN(NNClassifier):\n",
    "    \n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1,\n",
    "                 num_classes=512, fine_tuning=False):\n",
    "        super(CNN_RNN, self).__init__()\n",
    "        vgg = tv.models.vgg16_bn(pretrained=True)\n",
    "        for param in vgg.parameters():\n",
    "            param.requires_grad = fine_tuning\n",
    "        self.features = vgg.features\n",
    "        # the average pooling is the same\n",
    "        self.avgpool = vgg.avgpool\n",
    "        # the classifier is also the same\n",
    "        self.classifier = vgg.classifier\n",
    "        # CODE to change the final classifier layer\n",
    "        num_ftrs = vgg.classifier[6].in_features\n",
    "        self.classifier[6] = nn.Linear(num_ftrs, num_classes)\n",
    "        \n",
    "        # RNN Part\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_size)\n",
    "        self.unit = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, x, caps, lens):\n",
    "        # COMPLETE the forward prop\n",
    "        f = self.features(x)\n",
    "        f = self.avgpool(f)\n",
    "        f = torch.flatten(f, 1)\n",
    "        f = self.classifier(f)\n",
    "        \n",
    "        # RNN forward prop\n",
    "        embeddings = self.embeddings(caps)\n",
    "        inputs = torch.cat((f.unsqueeze(1), embeddings), 1)\n",
    "        packed_ip = pack_padded_sequence(inputs, lens, batch_first=True)\n",
    "        \n",
    "        h_state, _ = self.unit(packed_ip)\n",
    "        outputs = self.linear(h_state[0])\n",
    "        return outputs\n",
    "    \n",
    "    def greedy_sample(self, feats, max_len=30):\n",
    "        output_ids = []\n",
    "        states = None\n",
    "        inputs = feats.unsqueeze(1)\n",
    "\n",
    "        for i in range(max_len):\n",
    "            \n",
    "            h_state, states = self.unit(inputs, states)\n",
    "            outputs = self.linear(h_state.squeeze(1))\n",
    "            predicted = outputs.max(1)[1]\n",
    "            output_ids.append(predicted)\n",
    "            inputs = self.embeddings(predicted)\n",
    "            inputs = inputs.unsqueeze(1)\n",
    "            \n",
    "        output_ids = torch.stack(output_ids, 1)\n",
    "        return output_ids.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionStatsManager(nt.StatsManager):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CaptionStatsManager, self).__init__()\n",
    "        \n",
    "    def init(self):\n",
    "        super(CaptionStatsManager, self).init()\n",
    "        self.tokenized_true = {}\n",
    "        self.tokenized_pred = {}\n",
    "        self.scorer = Bleu(4)\n",
    "        self.running_bleu_scores = [0 for _ in range(4)]\n",
    "        \n",
    "    def accumulate(self, loss, x, y, d):\n",
    "        super(CaptionStatsManager, self).accumulate(loss, x, y, d)        \n",
    "        self.tokenized_true[0] = []\n",
    "        self.tokenized_pred[0] = []\n",
    "        _, pred_cap_lab = torch.max(y, 1)\n",
    "        true_cap_lab = d\n",
    "        pred_cap = index_to_cap(pred_cap_lab)\n",
    "        true_cap = index_to_cap(true_cap_lab)\n",
    "        self.tokenized_true[0].append(true_cap)\n",
    "        self.tokenized_pred[0].append(pred_cap)\n",
    "        bleu_scores, _ = self.scorer.compute_score(self.tokenized_true, self.tokenized_pred)\n",
    "        self.running_bleu_scores = list(map(add, self.running_bleu_scores, bleu_scores))\n",
    "        \n",
    "        \n",
    "    def summarize(self):\n",
    "        # this is the average loss when called\n",
    "        loss = super(CaptionStatsManager, self).summarize()\n",
    "        \n",
    "        # this is the average accuracy percentage when called\n",
    "        bleu_score = [ a / self.number_update for a in self.running_bleu_scores]\n",
    "        return {'loss' : loss, 'bleu' : bleu_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(exp, fig, axes):\n",
    "    axes[0].clear()\n",
    "    axes[1].clear()\n",
    "\n",
    "    # Plot the training loss over the epochs\n",
    "    axes[0].plot([valu['loss'] for valu in exp.history],\n",
    "                label=\"training loss\")\n",
    "\n",
    "    # legend for the plot\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # xlabel and ylabel\n",
    "    axes[0].set_xlabel(\"Number of Mini-batches\")\n",
    "    axes[0].set_ylabel(\"Loss\")\n",
    "    \n",
    "    # Plot the training accuracy over the epochs\n",
    "    axes[1].plot([valu['bleu'][0] for valu in exp.history],\n",
    "                label=\"training BLEU-1\")\n",
    "    axes[1].plot([valu['bleu'][1] for valu in exp.history],\n",
    "                label=\"training BLEU-2\")\n",
    "    axes[1].plot([valu['bleu'][2] for valu in exp.history],\n",
    "                label=\"training BLEU-3\")\n",
    "    axes[1].plot([valu['bleu'][3] for valu in exp.history],\n",
    "                label=\"training BLEU-4\")\n",
    "    \n",
    "    # legend for the plot\n",
    "    axes[1].legend()\n",
    "    \n",
    "    # xlabel and ylabel\n",
    "    axes[1].set_xlabel(\"Number of Mini-batches\")\n",
    "    axes[1].set_ylabel(\"BLEU Scores\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "net = CNN_RNN(embed_size=512, hidden_size=512, vocab_size=len(vocab),\n",
    "              num_layers=1, num_classes=512, fine_tuning=False)\n",
    "net = net.to(device)\n",
    "adam = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "stats_manager = CaptionStatsManager()\n",
    "exp1 = nt.Experiment(net, training_dataset, val_dataset, adam,\n",
    "                     stats_manager, collate_func=collate_function, output_dir=op_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, figsize=(7, 3))\n",
    "exp1.run(num_epochs=5, plot=lambda exp: plot(exp, fig=fig, axes=axes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(op_dir+'val_result.txt','a') as t_file:\n",
    "    op = t_file.read()\n",
    "    \n",
    "print(op)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
