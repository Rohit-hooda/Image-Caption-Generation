{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python2 and python3 compatibility between loaded modules\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All imports go here\n",
    "%matplotlib notebook\n",
    "\n",
    "# Reading files\n",
    "import os\n",
    "\n",
    "# Vector manipulations\n",
    "import numpy as np\n",
    "\n",
    "# DL framework\n",
    "# torch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch.utils.data as td\n",
    "import torchvision as tv\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "# Plotting images\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# COCO loading captions\n",
    "from pycocotools.coco import COCO\n",
    "import skimage.io as io\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (8.0, 10.0)\n",
    "\n",
    "# import created vocabulary\n",
    "from vocab_creator import VocabCreate as vc\n",
    "\n",
    "# PIL Image\n",
    "from PIL import Image\n",
    "\n",
    "# regex for captions\n",
    "import re\n",
    "\n",
    "# import nntools\n",
    "import nntools_modified as nt\n",
    "\n",
    "# import add for fast addition between lists\n",
    "from operator import add\n",
    "\n",
    "from pycocoevalcap.bleu.bleu import Bleu\n",
    "\"\"\"\n",
    "# evaluation metrics on MSCOCO dataset\n",
    "from pycocoevalcap.eval import COCOEvalCap\n",
    "from pycocoevalcap.tokenizer.ptbtokenizer import PTBTokenizer\n",
    "\n",
    "from pycocoevalcap.meteor.meteor import Meteor\n",
    "from pycocoevalcap.rouge.rouge import Rouge\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "from pycocoevalcap.spice.spice import Spice\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# json for dumping stuff onto files as output\n",
    "import json\n",
    "from json import encoder\n",
    "encoder.FLOAT_REPR = lambda o: format(o, '.3f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "dataset_root_dir = '/datasets/COCO-2015/'\n",
    "annotations_root_dir = '../datasets/COCO/annotations/'\n",
    "train_dir = \"train2014\"\n",
    "val_dir = \"val2014\"\n",
    "test_dir = \"test2015\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data annotations\n",
    "train_ann = \"{}captions_{}.json\".format(annotations_root_dir, train_dir)\n",
    "coco_train_caps = COCO(train_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation data annotations\n",
    "val_ann = \"{}captions_{}.json\".format(annotations_root_dir, val_dir)\n",
    "coco_val_caps = COCO(val_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the image IDs\n",
    "train_image_ids = coco_train_caps.getImgIds()\n",
    "# loadImgs() returns all the images\n",
    "train_imgs = coco_train_caps.loadImgs(train_image_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(train_imgs), len(train_imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the val image ids\n",
    "val_image_ids = coco_val_caps.getImgIds()\n",
    "# loadImgs() returns all the images\n",
    "val_imgs = coco_val_caps.loadImgs(val_image_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read 1 random training image from file \n",
    "rand_id = np.random.randint(0, len(train_imgs))\n",
    "rand_img = io.imread('{}/{}/{}'.format(dataset_root_dir, train_dir, train_imgs[rand_id]['file_name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.axis('off')\n",
    "plt.imshow(rand_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load caption for this particular image\n",
    "ann_id = coco_train_caps.getAnnIds(imgIds=train_imgs[rand_id]['id'])\n",
    "anns = coco_train_caps.loadAnns(ann_id)\n",
    "coco_train_caps.showAnns(anns)\n",
    "f = plt.figure()\n",
    "plt.imshow(rand_img)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(anns)\n",
    "print(len(anns))\n",
    "print(anns[0])\n",
    "print(anns[0]['caption'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read 1 random testing image from file\n",
    "val_rand_id = np.random.randint(0, len(val_imgs))\n",
    "val_rand_img = io.imread('{}/{}/{}'.format(dataset_root_dir, val_dir, val_imgs[val_rand_id]['file_name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.axis('off')\n",
    "plt.imshow(val_rand_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load caption for this particular image\n",
    "ann_id = coco_val_caps.getAnnIds(imgIds=val_imgs[val_rand_id]['id'])\n",
    "anns = coco_val_caps.loadAnns(ann_id)\n",
    "coco_val_caps.showAnns(anns)\n",
    "f = plt.figure()\n",
    "plt.imshow(val_rand_img)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset class\n",
    "class COCODataset(td.Dataset):\n",
    "    \n",
    "    \n",
    "    def __init__(self, dataset_root_dir, annotations_root_dir, vocab, mode=\"train2014\", image_size=(224, 224)):\n",
    "        super(COCODataset, self).__init__()\n",
    "        self.dataset_root_dir = dataset_root_dir\n",
    "        self.annotations_root_dir = annotations_root_dir\n",
    "        self.image_size = image_size\n",
    "        self.mode = mode\n",
    "        # training data annotations\n",
    "        self.ann = \"{}captions_{}.json\".format(annotations_root_dir, mode)\n",
    "        self.coco_caps = COCO(self.ann)\n",
    "        # get all the image IDs\n",
    "        self.image_ids = self.coco_caps.getImgIds()\n",
    "        self.ann_ids = list(self.coco_caps.anns.keys())\n",
    "        # loadImgs() returns all the images\n",
    "        self.imgs = self.coco_caps.loadImgs(self.image_ids)\n",
    "        self.vocab = vocab\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ann_ids)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"COCODataset(mode={}, image_size={})\". \\\n",
    "        format(self.mode, self.image_size)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        ann_id = self.ann_ids[idx]\n",
    "        cap = self.coco_caps.anns[ann_id][\"caption\"]\n",
    "        img_id = self.coco_caps.anns[ann_id][\"image_id\"]\n",
    "        img_path = self.coco_caps.loadImgs(img_id)[0][\"file_name\"]\n",
    "        \n",
    "        img = Image.open('{}/{}/{}'.format(self.dataset_root_dir, self.mode, img_path))\n",
    "        img = img.convert('RGB')\n",
    "        transform = tv.transforms.Compose([\n",
    "            tv.transforms.Resize(self.image_size),\n",
    "            #tv.transforms.RandomHorizontalFlip(),\n",
    "            tv.transforms.ToTensor(),\n",
    "            tv.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "        x = transform(img)\n",
    "        \n",
    "        # return caption\n",
    "        cap = str(cap)\n",
    "        clean_cap = re.sub(r'[^a-zA-Z0-9 ]+', '', cap)\n",
    "        word_list = clean_cap.lower().strip().split()\n",
    "        for i in range(len(word_list)):\n",
    "            if word_list[i] not in vocab.one_hot_inds:\n",
    "                word_list[i]=\"unk_vec\"\n",
    "        d = torch.Tensor([vocab.one_hot_inds[\"start_vec\"]]\n",
    "                               + [vocab.one_hot_inds[w] for w in word_list]\n",
    "                               + [vocab.one_hot_inds[\"end_vec\"]]\n",
    "        )\n",
    "        return x, d\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load the vocabulary\n",
    "# or Create and save to output\n",
    "dict_path = \"../outputs/vocab.npz\"\n",
    "vocab = vc(train_ann, dict_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of the cocodataset\n",
    "training_dataset = COCODataset(dataset_root_dir, annotations_root_dir, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myimshow(image, ax=plt):\n",
    "    image = image.to('cpu').numpy()\n",
    "    image = np.moveaxis(image, [0, 1, 2], [2, 0, 1])\n",
    "    image = (image + 1) / 2\n",
    "    image[image<0] = 0\n",
    "    image[image>1] = 1\n",
    "    h = ax.imshow(image)\n",
    "    ax.axis('off')\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, cap = training_dataset.__getitem__(47)\n",
    "print(cap)\n",
    "myimshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cap.numpy().astype(int)\n",
    "print(cap, cap.dtype)\n",
    "print(type(list(cap)))\n",
    "captions = [vocab.dict[cap[c]] for c in range(len(cap))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in range(len(cap)):\n",
    "    print(cap[c], type(cap[c]))\n",
    "    print(vocab.dict[cap[c]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" \".join(captions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = \"hellconvention\"\n",
    "clean_cap = re.sub(r'[^a-zA-Z0-9 ]+', '', cap)\n",
    "word_list = clean_cap.lower().strip().split()\n",
    "for i in range(len(word_list)):\n",
    "    if word_list[i] not in vocab.one_hot_inds:\n",
    "        word_list[i]=\"unk_vec\"\n",
    "\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the dataloader to be used\n",
    "# collate_fn - to pad all the vectors to the same length\n",
    "def collate_function(data):\n",
    "    data.sort(key=lambda x:len(x[1]), reverse=True)\n",
    "    img, cap=zip(*data)\n",
    "\n",
    "    #stack images\n",
    "    img = torch.stack(img, 0)\n",
    "\n",
    "    #concatenate all captions\n",
    "    cap_lens = [len(c) for c in cap]\n",
    "    max_cap_lens = max(cap_lens)\n",
    "    cap_lens = torch.Tensor(cap_lens)\n",
    "\n",
    "    #pad all captions to max caption length\n",
    "    padded_caption = torch.zeros(len(cap),max_cap_lens).long()\n",
    "    for i, c in enumerate(cap):\n",
    "        c_len = int(cap_lens[i].item())\n",
    "        padded_caption[i,:c_len] = c[:c_len]\n",
    "\n",
    "    return img, padded_caption, cap_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = td.DataLoader(training_dataset, batch_size=128, shuffle=True, pin_memory=True,\n",
    "                             collate_fn=collate_function, worker_init_fn=torch.manual_seed(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(train_loader), len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index to caption\n",
    "def index_to_cap(labs):\n",
    "    \"\"\"Index to caption\"\"\"\n",
    "    cap = labs.cpu().data.numpy().astype(int)\n",
    "    caps = [vocab.dict[cap[c]] for c in range(len(cap))]\n",
    "    #caps = caps[1:-1]\n",
    "    caps = list(filter(lambda a: a != \"start_vec\", caps))\n",
    "    caps = list(filter(lambda a: a != \"end_vec\", caps))\n",
    "    caption = \" \".join(caps)\n",
    "    return caption\n",
    "\n",
    "# index to caption\n",
    "def i2c(labs):\n",
    "    \"\"\"Index to caption\"\"\"\n",
    "    cap = labs.cpu().data.numpy().astype(int)\n",
    "    caps = [vocab.dict[cap[c]] for c in range(len(cap))]\n",
    "    #caps = caps[1:-1]\n",
    "    #caps = list(filter(lambda a: a != \"start_vec\", caps))\n",
    "    #caps = list(filter(lambda a: a != \"end_vec\", caps))\n",
    "    caption = \" \".join(caps)\n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display 1st image and label pair for the 1st 4 minibatches\n",
    "fig, axes = plt.subplots(ncols=2)\n",
    "fig.suptitle(\"1st image for 1st 2 minibatches\")\n",
    "\n",
    "for bind, mbat in enumerate(train_loader):\n",
    "    # print(len(mbat))\n",
    "    # print(type(mbat[0]), type(mbat[1]))\n",
    "    # print(mbat[0].size(), mbat[1].size())\n",
    "    \n",
    "    # The dataloader returns both the image and the caption\n",
    "    # image is in the 0th index, caption is 1st index\n",
    "    img = mbat[0][0, :, :, :]\n",
    "    lab = mbat[1][0]\n",
    "    capt = index_to_cap(lab)\n",
    "    capa = i2c(lab)\n",
    "    # print(lab.item())\n",
    "    myimshow(img, ax=axes[bind])\n",
    "    #axes[bind].text(50, 250, \"label: {}\".format(caption), size=12, verticalalignment='center')\n",
    "    # axes[bind].set_ylabel(\"label: {}\".format(lab.item()))\n",
    "    axes[bind].set_title(\"mini-batch {}\".format(bind+1))\n",
    "    print(capt)\n",
    "    print(capa)\n",
    "    if bind == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation dataset\n",
    "val_dataset = COCODataset(dataset_root_dir, annotations_root_dir, vocab, mode=val_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = td.DataLoader(val_dataset, batch_size=128, shuffle=False, pin_memory=True, collate_fn=collate_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN classifier from nntools\n",
    "class NNClassifier(nt.NeuralNetwork):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(NNClassifier, self).__init__()\n",
    "        self.cross_entropy = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def criterion(self, y, d):\n",
    "        return self.cross_entropy(y, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vgg = tv.models.vgg16_bn(pretrained=True)\n",
    "# print the network\n",
    "print(vgg)\n",
    "\n",
    "# print the named parameters of the network\n",
    "for name, param in vgg.named_parameters():\n",
    "    print(name, param.size(), param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_RNN(NNClassifier):\n",
    "    \n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1,\n",
    "                 num_classes=512, fine_tuning=False):\n",
    "        super(CNN_RNN, self).__init__()\n",
    "        vgg = tv.models.vgg16_bn(pretrained=True)\n",
    "        for param in vgg.parameters():\n",
    "            param.requires_grad = fine_tuning\n",
    "        self.features = vgg.features\n",
    "        # the average pooling is the same\n",
    "        self.avgpool = vgg.avgpool\n",
    "        # the classifier is also the same\n",
    "        self.classifier = vgg.classifier\n",
    "        # CODE to change the final classifier layer\n",
    "        num_ftrs = vgg.classifier[6].in_features\n",
    "        self.classifier[6] = nn.Linear(num_ftrs, num_classes)\n",
    "        \n",
    "        # RNN Part\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_size)\n",
    "        self.unit = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, x, caps, lens):\n",
    "        # COMPLETE the forward prop\n",
    "        f = self.features(x)\n",
    "        f = self.avgpool(f)\n",
    "        f = torch.flatten(f, 1)\n",
    "        f = self.classifier(f)\n",
    "        \n",
    "        # RNN forward prop\n",
    "        embeddings = self.embeddings(caps)\n",
    "        inputs = torch.cat((f.unsqueeze(1), embeddings), 1)\n",
    "        packed_ip = pack_padded_sequence(inputs, lens, batch_first=True)\n",
    "        \n",
    "        h_state, _ = self.unit(packed_ip)\n",
    "        outputs = self.linear(h_state[0])\n",
    "        return outputs\n",
    "    \n",
    "    def greedy_sample(self, feats, max_len=30):\n",
    "        output_ids = []\n",
    "        states = None\n",
    "        inputs = feats.unsqueeze(1)\n",
    "\n",
    "        for i in range(max_len):\n",
    "            \n",
    "            h_state, states = self.unit(inputs, states)\n",
    "            outputs = self.linear(h_state.squeeze(1))\n",
    "            predicted = outputs.max(1)[1]\n",
    "            output_ids.append(predicted)\n",
    "            inputs = self.embeddings(predicted)\n",
    "            inputs = inputs.unsqueeze(1)\n",
    "            \n",
    "        output_ids = torch.stack(output_ids, 1)\n",
    "        return output_ids.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionStatsManager(nt.StatsManager):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CaptionStatsManager, self).__init__()\n",
    "        \n",
    "    def init(self):\n",
    "        super(CaptionStatsManager, self).init()\n",
    "        self.tokenized_true = {}\n",
    "        self.tokenized_pred = {}\n",
    "        self.scorer = Bleu(4)\n",
    "        self.running_bleu_scores = [0 for _ in range(4)]\n",
    "        \n",
    "    def accumulate(self, loss, x, y, d):\n",
    "        super(CaptionStatsManager, self).accumulate(loss, x, y, d)        \n",
    "        self.tokenized_true[0] = []\n",
    "        self.tokenized_pred[0] = []\n",
    "        _, pred_cap_lab = torch.max(y, 1)\n",
    "        true_cap_lab = d\n",
    "        pred_cap = index_to_cap(pred_cap_lab)\n",
    "        true_cap = index_to_cap(true_cap_lab)\n",
    "        self.tokenized_true[0].append(true_cap)\n",
    "        self.tokenized_pred[0].append(pred_cap)\n",
    "        bleu_scores, _ = self.scorer.compute_score(self.tokenized_true, self.tokenized_pred)\n",
    "        self.running_bleu_scores = list(map(add, self.running_bleu_scores, bleu_scores))\n",
    "        \n",
    "        \n",
    "    def summarize(self):\n",
    "        # this is the average loss when called\n",
    "        loss = super(CaptionStatsManager, self).summarize()\n",
    "        \n",
    "        # this is the average accuracy percentage when called\n",
    "        bleu_score = [ a / self.number_update for a in self.running_bleu_scores]\n",
    "        return {'loss' : loss, 'bleu' : bleu_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(exp, fig, axes):\n",
    "    axes[0].clear()\n",
    "    axes[1].clear()\n",
    "    \n",
    "#     print(\"In plot()\")\n",
    "#     print(len(exp.history))\n",
    "#     print(exp.history)\n",
    "#     print(exp.history[0])\n",
    "#     print(exp.history[0]['loss'])\n",
    "#     print(exp.history[0]['bleu'])\n",
    "#     print(exp.history[0]['bleu'][3])\n",
    "    #print(exp.epoch)\n",
    "    \n",
    "    # Plot the training loss over the epochs\n",
    "    axes[0].plot([exp.history[k]['loss'] for k in range((exp.epoch[0] + 1)*(exp.epoch[1]))],\n",
    "                label=\"training loss\")\n",
    "    \n",
    "    # Plot the evaluation loss over the epochs\n",
    "    #axes[0].plot([exp.history[k][1]['loss'] for k in range(exp.epoch)],\n",
    "    #            color='orange', label=\"evaluation loss\")\n",
    "    \n",
    "    # legend for the plot\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # xlabel and ylabel\n",
    "    axes[0].set_xlabel(\"Number of Mini-batches\")\n",
    "    axes[0].set_ylabel(\"Loss\")\n",
    "    \n",
    "    # Plot the training accuracy over the epochs\n",
    "    axes[1].plot([exp.history[k]['bleu'][0] for k in range((exp.epoch[0] + 1)*(exp.epoch[1]))],\n",
    "                label=\"training BLEU-1\")\n",
    "    axes[1].plot([exp.history[k]['bleu'][1] for k in range((exp.epoch[0] + 1)*(exp.epoch[1]))],\n",
    "                label=\"training BLEU-2\")\n",
    "    axes[1].plot([exp.history[k]['bleu'][2] for k in range((exp.epoch[0] + 1)*(exp.epoch[1]))],\n",
    "                label=\"training BLEU-3\")\n",
    "    axes[1].plot([exp.history[k]['bleu'][3] for k in range((exp.epoch[0] + 1)*(exp.epoch[1]))],\n",
    "                label=\"training BLEU-4\")\n",
    "    \n",
    "    # Plot the evaluation accuracy over the epochs\n",
    "    #axes[1].plot([exp.history[k][1]['accuracy'] for k in range(exp.epoch)],\n",
    "    #            color='orange', label=\"evaluation accuracy\")\n",
    "    \n",
    "    # legend for the plot\n",
    "    axes[1].legend()\n",
    "    \n",
    "    # xlabel and ylabel\n",
    "    axes[1].set_xlabel(\"Number of Minibatches\")\n",
    "    axes[1].set_ylabel(\"BLEU Scores\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # set the title for the figure\n",
    "    # fig.suptitle(\"Loss and Accuracy metrics\")\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "net = CNN_RNN(embed_size=512, hidden_size=512, vocab_size=len(vocab),\n",
    "              num_layers=1, num_classes=512, fine_tuning=False)\n",
    "net = net.to(device)\n",
    "adam = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "stats_manager = CaptionStatsManager()\n",
    "exp1 = nt.Experiment(net, training_dataset, val_dataset, adam,\n",
    "                     stats_manager, collate_func=collate_function, output_dir=\"../outputs/framework_test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, figsize=(7, 3))\n",
    "exp1.run(num_epochs=1, plot=lambda exp: plot(exp, fig=fig, axes=axes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp1_val = exp1.evaluate()\n",
    "print(exp1_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
