# README - learning_stuff

### Overview

This folder is to test out some small code examples to learn and understand the concepts that are used for this project

### Contents

The contents of this folder are as follows.
1. min_char_rnn.py - This file contains code to run and train a minimal vanilla RNN on characters. The original code is found [here](https://gist.github.com/karpathy/d4dee566867f8291f086).
2. input.txt - This file is the input to the RNN model that we shall train. For our case, we gather the file that Andrej Karpathy has linked. The file can be found [here](https://cs.stanford.edu/people/karpathy/char-rnn/pg.txt)

NOTE!!
The input.txt is not the actual dataset, but just a sampler generated by the RNN trained by Andrej Karpathy on an original dataset

### Results
Running the vanilla RNN code, after modifying it to python3 we get the following results.
The data has 49993 characters, 74 unique.

Iteration vs loss is as follows:-

| Iteration | Loss |
| :---: | :---: |
iter 0 | loss: 107.601616
iter 500 | loss: 95.172854
iter 1000 | loss: 81.358349
iter 1500 | loss: 71.627678
iter 2000 | loss: 64.308998
iter 2500 | loss: 59.543744
iter 3000 | loss: 55.602259
iter 3500 | loss: 53.026307
iter 4000 | loss: 50.767121
iter 4500 |loss: 49.396412
iter 5000 | loss: 47.935096
iter 5500 | loss: 47.071578
iter 6000 | loss: 46.069663
iter 6500 | loss: 45.472349
iter 7000 | loss: 44.636500
iter 7500 | loss: 44.244871
iter 8000 | loss: 43.673769
iter 8500 | loss: 43.322208
iter 9000 | loss: 42.733480
iter 9500 | loss: 42.477981
iter 10000 | loss: 42.119052
iter 10500 | loss: 41.845682
iter 11000 | loss: 41.394694
iter 11500 | loss: 41.202135
iter 12000 | loss: 40.942144
iter 12500 | loss: 40.713392

The loss is indeed decreasing as the number of iterations increases.


Qualitatively, the initial randomly generated text was as follows:-
> GBtPa8Fb:MTr9m—dBjniVCTfmY $T?.Ef.a1sW Bwh2K_GPjCAp] 4lgNoMdbmWf;_LRTw$C-aV1rTa;0$B(vTCLgMvonr8jCFzK_CYou1g—e'R2;[_tKhOvw(vHMBkTqlC0M1c2;e[drqto?Ro]5W.ls_nY]wb[A8MCv1?AFIu%4EScn$iY42eA4zliNHs2kzB[2tr ]])])

This is the text generated after 5000 iterations
> tTarsun ipvers in muke lactwyo than bitekione got is a dould inctad favermonting, sutrem cinvertoing here hat That net of rell aveg they for invekime youl if but geals cand ingecand af chad ably but b 

This is the text generated after 10000 iterations
> what usorst and in whing the when wher 't f che butart tial inves. I'ting one thoy who to in prof of they qu ig the  rowming a being uso ray A comping in con a liod have tour, you're part and sare aif

This is the text that was generated finally after 12500 iterations
>  ecaung to be the seat for whet to be startupon't of compootrituly going, su]ped on that the oof A worka tow is orte forlisimes you're for in the comping ther the sored a larsing the lion to go ane moo
### Acknowledgements

[This](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) is the excellent blog-post by @karpathy that points to this code. This blog post explains RNNs in a beautiful and succint way.

